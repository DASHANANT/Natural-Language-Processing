{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class SimplifiedLesk: \n",
    "\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    def disambiguate(self, word, sentence):       \n",
    "        word_senses = wordnet.synsets(word)\n",
    "        best_sense = word_senses[0]  # Assume that first sense is most freq.\n",
    "        max_overlap = 0\n",
    "        context = set(word_tokenize(sentence))\n",
    "        for sense in word_senses:\n",
    "            signature = self.tokenized_gloss(sense)\n",
    "            overlap = self.compute_overlap(signature, context)            \n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                best_sense = sense\n",
    "        return best_sense  \n",
    "    \n",
    "    \n",
    "    def tokenized_gloss(self, sense):        \n",
    "        tokens = set(word_tokenize(sense.definition()))\n",
    "        for example in sense.examples():\n",
    "            tokens.union(set(word_tokenize(example)))\n",
    "        return tokens\n",
    "\n",
    "    def compute_overlap(self, signature, context):       \n",
    "        sig = signature.difference(self.stopwords)\n",
    "        return len(sig.intersection(context))\n",
    "\n",
    "\n",
    "import re\n",
    "functionwords = {'everyone', 'himself', 'it', 'his', 'everything', 'little', 'those', 'inside', 'on', 'off', 'over', \n",
    "                 'of', 'first', 'within', 'around', 'near', 'so', 'would', 'else', 'for', 'moreover', 'besides', \n",
    "                 'into', 'while', 'here', 'never', 'such', 'each', 'who', 'anyone', 'through', 'despite', 'might',\n",
    "                 'that', 'will', 'anything', 'in', 'therefore', 'your', 'someone', 'a', 'few', 'do', 'second', 'down',\n",
    "                 'themself', 'usually', 'one', 'with', 'any', 'onto', 'all', 'to', 'must', 'herself', 'him', 'most', 'much',\n",
    "                 'but', 'along', 'should', 'my', 'an', 'no', 'against', 'before', 'could', 'now', 'there', 'meanwhile',\n",
    "                 'be', 'instead', 'during', 'them', 'from', 'less', 'if', 'something', 'ones', 'he', 'two', 'sometimes',\n",
    "                 'yours', 'have', 'however', 'otherwise', 'its', 'though', 'often', 'toward', 'than', 'their', 'then',\n",
    "                 'half', 'least', 'although', 'nothing', 'her', 'next', 'as', 'across', 'always', 'many', 'how', 'anyway',\n",
    "                 'when', 'this', 'behind', 'own', 'both', 'at', 'itself', 'last', 'hers', 'other', 'they', 'our',\n",
    "                 'incidentally', 'may', 'whose', 'beside', 'without', 'about', 'she', 'some', 'where', 'can', 'and',\n",
    "                 'because', 'every', 'theirs', 'twice', 'another', 'since', 'what', 'after', 'which', 'these', 'more',\n",
    "                 'shall', 'by', 'several', 'the', 'or'}\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "\n",
    "class UpdatedLesk: \n",
    "\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.stopwords.update(functionwords)         \n",
    "        self.pos=None\n",
    "\n",
    "    def disambiguate(self, word, pos, sentence):       \n",
    "        word_senses = wordnet.synsets(word)        \n",
    "        word_senses1=[]        \n",
    "        for i in word_senses:\n",
    "            if i.pos()==pos:\n",
    "                word_senses1.append(i) \n",
    "                \n",
    "        best_sense = word_senses1[0]  # Assume that first sense is most freqent\n",
    "        max_overlap = 0\n",
    "        context = set(word_tokenize(sentence))        \n",
    "        for sense in word_senses1:            \n",
    "            signature = self.tokenized_gloss(sense)\n",
    "            overlap= self.overlapcontext(sense,signature, str(sentence))\n",
    "            if overlap > max_overlap:\n",
    "                    max_overlap = overlap\n",
    "                    best_sense = sense            \n",
    "       \n",
    "        return best_sense.definition() \n",
    "            \n",
    "       \n",
    "    def find(self, sentence):\n",
    "        context = set(word_tokenize(sentence))\n",
    "        context=list(context)\n",
    "        for i in range(len(context)):\n",
    "            context[i]=str(self.stem(context[i])) \n",
    "        context=set(context)\n",
    "        context = context.difference(self.stopwords)\n",
    "        z=[]\n",
    "        for i in context:            \n",
    "            if len(wordnet.synsets(i))>1:\n",
    "                z.append(i)\n",
    "        return z\n",
    "    \n",
    "    def change(self,i): \n",
    "        if i=='VB' or i=='VBD' or i=='VBG' or i=='VBN' or i=='VBP' or i=='VBZ' :\n",
    "            return 'v'\n",
    "        elif i== 'NN' or i=='NNS' or i=='NNP' or i=='NNPS' :\n",
    "            return 'n'\n",
    "        elif i== 'RB'or i=='RBR' or i=='RBS' or i=='RP':\n",
    "            return 'r'\n",
    "        elif i=='JJ'or i=='JJR'or i=='JJS':\n",
    "            return 's'\n",
    "        else:\n",
    "            return 'a' \n",
    "        \n",
    "    def stem(self,i):\n",
    "        if wordnet.synsets(ps.stem(i))==wordnet.synsets(i):\n",
    "            return ps.stem(i)\n",
    "        else:\n",
    "            return i\n",
    "        \n",
    "        \n",
    "    \n",
    "    def findpos(self, sentence):\n",
    "        t1 = nltk.word_tokenize(sentence)\n",
    "        tag1 = nltk.pos_tag(t1)\n",
    "        context=[]\n",
    "        for i in tag1:\n",
    "            context.append(i[0])\n",
    "        context = set(word_tokenize(sentence))        \n",
    "        context = context.difference(self.stopwords)        \n",
    "        tag11=[]\n",
    "        for k in context:\n",
    "            for i in tag1:\n",
    "                if i[0]==k:\n",
    "                    tag11.append(i)        \n",
    "        \n",
    "        tag11=np.array(tag11)       \n",
    "        for i in range(len(tag11)):\n",
    "            tag11[i][1]=str(self.change(tag11[i][1])) \n",
    "            \n",
    "        for i in range(len(tag11)):\n",
    "            tag11[i][0]=str(self.stem(tag11[i][0])) \n",
    "                \n",
    "        return tag11\n",
    "    \n",
    "    def tokenized_gloss(self, sense):        \n",
    "        tokens = set(word_tokenize(sense.definition()))\n",
    "        tokens=list(tokens)\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i]=str(self.stem(tokens[i])) \n",
    "        tokens=set(tokens)\n",
    "        for example in sense.examples():\n",
    "            tokens.union(set(word_tokenize(example)))\n",
    "        return tokens\n",
    "    \n",
    "    def tokenized_sent(self, k):\n",
    "        string=str(k)\n",
    "        tokens = set(word_tokenize(string))\n",
    "        tokens=list(tokens)\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i]=str(self.stem(tokens[i])) \n",
    "        tokens=set(tokens)        \n",
    "        return tokens\n",
    "    \n",
    "    def overlapcontext(self, synset, glos, sentence):        \n",
    "        gloss=glos                    \n",
    "        for m in synset.hypernyms(): \n",
    "            string = re.sub('[^a-zA-Z.\\d\\s]', '', str(m.definition()))\n",
    "            m1=self.tokenized_sent(string)\n",
    "            gloss=gloss.union(m1)\n",
    "            string1 = re.sub('[^a-zA-Z.\\d\\s]', '', str(m.examples()))\n",
    "            x1=self.tokenized_sent(string1)\n",
    "            gloss=gloss.union(x1)        \n",
    "            \n",
    "        for z in synset.hyponyms():\n",
    "            string = re.sub('[^a-zA-Z.\\d\\s]', '', str(z.definition()))\n",
    "            m1=self.tokenized_sent(string)\n",
    "            gloss=gloss.union(m1)\n",
    "            string1 = re.sub('[^a-zA-Z.\\d\\s]', '', str(z.examples()))\n",
    "            x1=self.tokenized_sent(string1)\n",
    "            gloss=gloss.union(x1)  \n",
    "\n",
    "        for k in synset.part_meronyms():        \n",
    "            string = re.sub('[^a-zA-Z.\\d\\s]', '', str(k.definition()))\n",
    "            m1=self.tokenized_sent(string)\n",
    "            gloss=gloss.union(m1)\n",
    "            string1 = re.sub('[^a-zA-Z.\\d\\s]', '', str(k.examples()))\n",
    "            x1=self.tokenized_sent(string1)\n",
    "            gloss=gloss.union(x1) \n",
    "         \n",
    "        for w in synset.substance_meronyms():        \n",
    "            string = re.sub('[^a-zA-Z.\\d\\s]', '', str(w.definition()))\n",
    "            m1=self.tokenized_sent(string)\n",
    "            gloss=gloss.union(m1)\n",
    "            string1 = re.sub('[^a-zA-Z.\\d\\s]', '', str(w.examples()))\n",
    "            x1=self.tokenized_sent(string1)\n",
    "            gloss=gloss.union(x1) \n",
    "        \n",
    "        for q in synset.member_meronyms():        \n",
    "            string = re.sub('[^a-zA-Z.\\d\\s]', '', str(q.definition()))\n",
    "            m1=self.tokenized_sent(string)\n",
    "            gloss=gloss.union(m1)\n",
    "            string1 = re.sub('[^a-zA-Z.\\d\\s]', '', str(q.examples()))\n",
    "            x1=self.tokenized_sent(string1)\n",
    "            gloss=gloss.union(x1)  \n",
    "        \n",
    "        gloss = gloss.difference(self.stopwords)        \n",
    "        sentence =self.tokenized_sent(sentence)\n",
    "        sentence = sentence.difference(self.stopwords)\n",
    "        return len( gloss.intersection(sentence)) \n",
    "    \n",
    "    \n",
    "import tkinter as tk\n",
    "def get_Words():\n",
    "        sentence = utext.get('1.0', \"end\").strip().lower() \n",
    "        \n",
    "        model=UpdatedLesk()\n",
    "        z=model.find(sentence)\n",
    "        AW.config(state='normal') \n",
    "        AW.delete('1.0','end')\n",
    "        AW.insert('1.0', z)\n",
    "        AW.config(state='disabled')\n",
    "\n",
    "def get_sense():\n",
    "        sentence = utext.get('1.0', \"end\").strip().lower()       \n",
    "        word = Word.get('1.0', \"end\").strip()    \n",
    "        \n",
    "        model=SimplifiedLesk()\n",
    "        sense=model.disambiguate(word,sentence)         \n",
    "        \n",
    "        summary.config(state='normal') \n",
    "        summary.delete('1.0','end')\n",
    "        summary.insert('1.0', sense.definition())        \n",
    "\n",
    "        summary.config(state='disabled')\n",
    "        \n",
    "def get_updatedsense():\n",
    "        sentence = utext.get('1.0', \"end\").strip().lower()       \n",
    "        word = Word.get('1.0', \"end\").strip()  \n",
    "        up=UpdatedLesk()\n",
    "        pos=up.findpos(sentence)        \n",
    "        poss.config(state='normal')\n",
    "        poss.delete('1.0','end')\n",
    "        poss.insert('1.0', pos) \n",
    "        poss.config(state='disabled')\n",
    "        pos=list(pos)\n",
    "        \n",
    "        k = None\n",
    "        for i in pos:\n",
    "            if i[0]== word:\n",
    "                k=i[1]\n",
    "                        \n",
    "        sense=up.disambiguate(word,k,sentence)         \n",
    "        summary1.config(state='normal') \n",
    "        summary1.delete('1.0','end')\n",
    "        summary1.insert('1.0', sense)\n",
    "        summary1.config(state='disabled')\n",
    "\n",
    "        \n",
    "        \n",
    "root = tk.Tk()\n",
    "root.title('Word Sense Predictor')\n",
    "root.geometry('800x650')\n",
    "# root['bg']='green'\n",
    "\n",
    "ulabel = tk.Label(root, text='Give a Sentence',font='ar 15 bold')\n",
    "ulabel.pack()\n",
    "utext = tk.Text(root, height=1.5, width=80)\n",
    "utext.pack()\n",
    "\n",
    "b1 = tk.Button(root, text='Get_Words',width=10, height=1, command=get_Words,font='ar 10 bold')\n",
    "b1.pack()\n",
    "\n",
    "wlabel = tk.Label(root, text='Ambiguous words',font='ar 10 bold')\n",
    "wlabel.pack()\n",
    "AW= tk.Text(root, height=4, width=80)\n",
    "AW.config(state='disabled', bg='#dddddd')\n",
    "AW.pack()\n",
    "\n",
    "alabel = tk.Label(root, text='Word To be disambiguated',font='ar 12 bold')\n",
    "alabel.pack()\n",
    "Word = tk.Text(root, height=1.5, width=20)\n",
    "Word.pack()\n",
    "\n",
    "slabel = tk.Label(root, text='Sense with Lesk Algorithm ',font='ar 15 bold')\n",
    "slabel.pack()\n",
    "summary = tk.Text(root, height=3, width=80)\n",
    "summary.config(state='disabled', bg='#dddddd')\n",
    "summary.pack()\n",
    "\n",
    "\n",
    "b2 = tk.Button(root, text='Get_Sense', width=20, height=1,command=get_sense,font='ar 10 bold')\n",
    "b2.pack()\n",
    "\n",
    "plabel = tk.Label(root, text='POS tags',font='ar 15 bold')\n",
    "plabel.pack()\n",
    "poss= tk.Text(root, height=4, width=80)\n",
    "poss.config(state='disabled', bg='#dddddd')\n",
    "poss.pack()\n",
    "\n",
    "\n",
    "s1label = tk.Label(root, text='Sense with Updated Lesk Algorithm',font='ar 15 bold')\n",
    "s1label.pack()\n",
    "summary1 = tk.Text(root, height=3, width=80)\n",
    "summary1.config(state='disabled', bg='#dddddd')\n",
    "summary1.pack()\n",
    "\n",
    "\n",
    "b3 = tk.Button(root, text='Get_Updated_Sense ', width=20, height=1, command=get_updatedsense,font='ar 10 bold')\n",
    "b3.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "def stem(i):\n",
    "    if wordnet.synsets(ps.stem(i))==wordnet.synsets(i):\n",
    "        return ps.stem(i)\n",
    "    else:\n",
    "        return i\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'was', 'cutting', 'the', 'carrot'] \n",
      " ['he', 'was', 'cutting', 'the', 'carrot']\n"
     ]
    }
   ],
   "source": [
    "input1='he was cutting the carrot'\n",
    "tokens = word_tokenize(input1)\n",
    "tokenss=[]\n",
    "for i in range(len(tokens)):\n",
    "    tokenss.append(str(stem(tokens[i])))\n",
    "\n",
    "\n",
    "    \n",
    "print(tokens,'\\n',tokenss)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example:\n",
    "    1)She chopped the vegetables with a chef’s knife.\n",
    "    knife----(tool)\n",
    "    2)A man was beaten and cut with a knife.\n",
    "    knife----(weapon) \n",
    "    \n",
    "    3)He deposited a lot of money in the bank\n",
    "    bank - a finiancial institute \n",
    "    4)A bank of clouds was building to the northeast.\n",
    "    bank--sloping land (especially the slope beside a body of water)\n",
    "    \n",
    "    5)At cricket he was equally good as a bat and as a wicket-keeper.\n",
    "    cricket- game\n",
    "    6)cricket is the insect that creates the chirping noise heard in rural areas at night.\n",
    "    cricket- insect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
