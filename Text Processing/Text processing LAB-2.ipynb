{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing LAB-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings as Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line no 0\n",
      "Forced to flee England, the Andrews family books passage to a fresh start in a distant country, only to discover a barren, inhospitable land at the end of their crossing. \n",
      "\n",
      "line no 1\n",
      "To seventeen-year-old Lavinia, uprooted from everything familiar, it seems a fate worse than the one they left behind. \n",
      "\n",
      "line no 2\n",
      "Driven by loneliness she begins a journal.\n"
     ]
    }
   ],
   "source": [
    "with open('f2','r') as file:\n",
    "    lines = file.readlines()\n",
    "for i in range(len(lines)):\n",
    "    print(\"line no\",i)\n",
    "    print(lines[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File as a String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Forced to flee England, the Andrews family books passage to a fresh start in a distant country, only to discover a barren, inhospitable land at the end of their crossing.To seventeen-year-old Lavinia, uprooted from everything familiar, it seems a fate worse than the one they left behind.Driven by loneliness she begins a journal.\n"
     ]
    }
   ],
   "source": [
    "with open( 'f2' , 'r' ) as BigFile:\n",
    "    data=BigFile. read(). replace(' \\n' , '' )         #print as a one paragaph\n",
    "# Verify the string type\n",
    "print(type(data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python - Filter Duplicate Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'has', 'colour', 'also', 'the', '.', 'The', 'Rainbow', 'blue', 'Sky', 'a', 'ocean']\n"
     ]
    }
   ],
   "source": [
    "word_data = \"The Sky is blue also the ocean is blue also Rainbow has a blue colour.\"\n",
    "# First Word tokenization\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "no_order = list(set(nltk_tokens))\n",
    "print(no_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preserving the Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Sky',\n",
       " 'is',\n",
       " 'blue',\n",
       " 'also',\n",
       " 'the',\n",
       " 'ocean',\n",
       " 'Rainbow',\n",
       " 'has',\n",
       " 'a',\n",
       " 'colour',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_tokens = set()\n",
    "order = [ ]\n",
    "for i in nltk_tokens:\n",
    "    if i not in ordered_tokens:\n",
    "        ordered_tokens.add(i)\n",
    "        order.append(i)\n",
    "order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions (RegEx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python - Extract Emails from Text\n",
    "### findall() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contact@tutorialspoint.com', 'feedback@tp.com']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "e1 = \"Please contact us at contact@tutorialspoint.com for further information.You can also give feedbacl at feedback@tp.com\"\n",
    " #define the pattern of an email ID\n",
    "email = re.findall(r\"[aa-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", e1)  \n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The First sentence is about Python.', 'The Second: about Django.', 'You can learn Python,Django and Data Ananlysis here.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The First sentence is about Python. The Second: about Django. You can learn Python,Django and Data Ananlysis here. \"\n",
    "nltk_tokens = nltk. sent_tokenize(sentence)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wie geht es Ihnen?', 'Gut, danke.']\n"
     ]
    }
   ],
   "source": [
    "german_tokenizer = nltk. data. load('tokenizers/punkt/german.pickle' )\n",
    "\n",
    "# german language\n",
    "\n",
    "german_tokens=german_tokenizer. tokenize('Wie geht es Ihnen? Gut, danke.' )\n",
    "print(german_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'originated', 'from', 'the', 'idea', 'that', 'there', 'are', 'readers', 'who', 'prefer', 'learning', 'new', 'skills', 'from', 'the', 'comforts', 'of', 'their', 'drawing', 'rooms']\n"
     ]
    }
   ],
   "source": [
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "  # separate words\n",
    "nltk_tokens = nltk. word_tokenize(word_data)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anant\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xeyr', 'ya', 'yalnız', 'yaxşı', 'yeddi', 'yenə', 'yəni', 'yetmiş', 'yox', 'yoxdur', 'yoxsa', 'yüz', 'zamanog', 'i', 'jeg', 'det', 'at', 'en', 'den', 'til', 'er', 'som', 'på', 'de', 'med', 'han', 'af', 'for', 'ikke', 'der', 'var', 'mig', 'sig', 'men', 'et', 'har', 'om', 'vi', 'min', 'havde', 'ham', 'hun', 'nu', 'over', 'da', 'fra', 'du', 'ud', 'sin', 'dem']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk. download('stopwords' )\n",
    "from nltk. corpus import stopwords\n",
    "stopwords. words('english' )\n",
    "print(stopwords. words() [ 400: 450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords. fileids())\n",
    "\n",
    "#no. of languauge present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python - Remove Stopwords\n",
    "Stopwords are the English words which does not add much meaning to a\n",
    "sentence. They can safely be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at']\n"
     ]
    }
   ],
   "source": [
    "from nltk. corpus import stopwords\n",
    "stopwords. words('english')\n",
    "print(stopwords. words() [620:680])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There\n",
      "tree\n",
      "near\n",
      "river\n"
     ]
    }
   ],
   "source": [
    "from nltk. corpus import stopwords\n",
    "en_stops = set(stopwords. words('english' ))\n",
    "all_words = [ 'There' , 'is' , 'a' , 'tree' , 'near' , 'the' , 'river' ]\n",
    "for word in all_words:\n",
    "    if word not in en_stops:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python - Synonyms and Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms\n",
      "Antonyms\n",
      "available\n",
      "part\n",
      "wordnet\n",
      "lexical\n",
      "database\n",
      "English\n",
      "language\n",
      ".\n",
      "It\n",
      "available\n",
      "part\n",
      "nltk\n",
      "corpora\n",
      "access\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "a = \"Synonyms and Antonyms are available as part of the wordnet which a lexical database for the English language. It is available as part of nltk corpora access.\"\n",
    "token = nltk.word_tokenize(a)\n",
    "for word in token:\n",
    "    if word not in en_stops:\n",
    "        print(word)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grease', 'territory', 'soil', 'ground', 'land', 'colly', 'bemire', 'dirty', 'filth', 'stain', 'grunge', 'grime', 'begrime', 'dirt'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Anant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "synonyms=[]\n",
    "for syn in wordnet. synsets(\"Soil\"):\n",
    "    for lm in syn. lemmas():\n",
    "        synonyms. append(lm. name())\n",
    "print (set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backward', 'back'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"ahead\"):\n",
    " for lm in syn.lemmas():\n",
    "     if lm.antonyms():\n",
    "         antonyms.append(lm.antonyms()[0].name())\n",
    "print(set(antonyms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python - Spelling Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in f:\\software\\lib\\site-packages (0.5.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspellchecker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "{'wak', 'walk', 'alak', 'wlat', 'weak', 'flak', 'blak'}\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown(['let', 'us', 'wlak','on','the','groun'])\n",
    "for word in misspelled:\n",
    " # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    " # Get a list of `likely` options\n",
    "    print(spell.candidates(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in f:\\software\\lib\\site-packages (0.5.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "{'Let'}\n",
      "us\n",
      "{'us'}\n",
      "walk\n",
      "{'wak', 'walk', 'alak', 'wlat', 'weak', 'flak', 'blak'}\n",
      "on\n",
      "{'on'}\n",
      "the\n",
      "{'the'}\n",
      "groun\n",
      "{'groun'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spell = SpellChecker()\n",
    "# find those words that may be misspelled\n",
    "misspelled = [ 'Let' , 'us' , 'wlak' , 'on' , 'the' , 'groun' ]\n",
    "for word in misspelled:\n",
    " # Get the one `most likely` answer\n",
    "    print( spell. correction(word))\n",
    " # Get a list of `likely` options\n",
    "    print( spell. candidates(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python - WordNet Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Anant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['locomotive', 'engine', 'locomotive_engine', 'railway_locomotive']\n",
      "a large body of water constituting a principal part of the hydrosphere\n",
      "a tall perennial woody plant having a main trunk and branches forming a distinct elevated crown; includes both gymnosperms and angiosperms\n"
     ]
    }
   ],
   "source": [
    "from nltk. corpus import wordnet as wn\n",
    "\n",
    "              ## All leemas\n",
    "    \n",
    "res=wn. synset('locomotive.n.1' ). lemma_names()\n",
    "print(res)\n",
    "         ## Defination of  word from dictionary\n",
    "resdef = wn. synset('ocean.n.1' ). definition()\n",
    "print(resdef)\n",
    "\n",
    "resdef1 = wn. synset('tree.n.1' ). definition()\n",
    "print(resdef1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a bad report card', 'his sloppy appearance made a bad impression', 'a bad little boy', 'clothes in bad shape', 'a bad cut', 'bad luck', 'the news was very bad', 'the reviews were bad', 'the pay is bad', 'it was a bad light for reading', 'the movie was a bad choice']\n"
     ]
    }
   ],
   "source": [
    "#Usage of words in Examples\n",
    "res_exm = wn. synset('bad.a.01' ). examples()\n",
    "#.n part of speech (adverb)\n",
    "print(res_exm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for your own good', \"what's the good of worrying?\"]\n"
     ]
    }
   ],
   "source": [
    "res_exm = wn. synset('good.n.01' ). examples()\n",
    "#.n part of speech (noun)\n",
    "print(res_exm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python - Corpora Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Anant\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "fields = gutenberg.fileids()\n",
    "print(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Poems by William Blake 1789]\n",
      "\n",
      " \n",
      "SONGS OF INNOCENCE AND OF EXPERIENCE\n",
      "and THE BOOK of THEL\n",
      "\n",
      "\n",
      " SONGS OF INNOCENCE\n",
      " \n",
      " \n",
      " INTRODUCTION\n",
      " \n",
      " Piping down the valleys wild,\n",
      "   Piping songs of pleasant glee,\n",
      " On a cloud I saw a child,\n",
      "   And he laughing said to me:\n",
      " \n",
      " \"Pipe a song about a Lamb!\"\n",
      "So I piped with merry cheer.\n",
      "\"Piper, pipe that song again;\"\n",
      "   So I piped: he wept to hear.\n",
      "\"Drop thy pipe, thy happy pipe;\n",
      "   Sing thy songs of happy cheer:!\"\n"
     ]
    }
   ],
   "source": [
    "from nltk. tokenize import sent_tokenize\n",
    "sample = gutenberg.raw(\"blake-poems.txt\")\n",
    "token = sent_tokenize(sample)\n",
    "for para in range(4):\n",
    "    print( token[para])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python - Tagging Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Anant\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('Python', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('serpent', 'NN'), ('which', 'WDT'), ('eats', 'VBZ'), ('eggs', 'NNS'), ('from', 'IN'), ('the', 'DT'), ('nest', 'JJS')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "text = nltk. word_tokenize(\"A Python is a serpent which eats eggs from the nest\")\n",
    "tagged_text=nltk. pos_tag(text)\n",
    "print(tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Anant\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk. help. upenn_tagset( 'NN' )\n",
    "nltk. help. upenn_tagset( 'IN' )\n",
    "nltk. help. upenn_tagset( 'DT' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n"
     ]
    }
   ],
   "source": [
    "nltk. help. upenn_tagset( 'VBZ' )\n",
    "nltk. help. upenn_tagset( 'JJS' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk. tokenize import sent_tokenize\n",
    "from nltk. corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'JJ'), ('Poems', 'NNP'), ('by', 'IN'), ('William', 'NNP'), ('Blake', 'NNP'), ('1789', 'CD'), (']', 'NNP'), ('SONGS', 'NNP'), ('OF', 'NNP'), ('INNOCENCE', 'NNP'), ('AND', 'NNP'), ('OF', 'NNP'), ('EXPERIENCE', 'NNP'), ('and', 'CC'), ('THE', 'NNP'), ('BOOK', 'NNP'), ('of', 'IN'), ('THEL', 'NNP'), ('SONGS', 'NNP'), ('OF', 'NNP'), ('INNOCENCE', 'NNP'), ('INTRODUCTION', 'NNP'), ('Piping', 'VBG'), ('down', 'RP'), ('the', 'DT'), ('valleys', 'NN'), ('wild', 'JJ'), (',', ','), ('Piping', 'NNP'), ('songs', 'NNS'), ('of', 'IN'), ('pleasant', 'JJ'), ('glee', 'NN'), (',', ','), ('On', 'IN'), ('a', 'DT'), ('cloud', 'NN'), ('I', 'PRP'), ('saw', 'VBD'), ('a', 'DT'), ('child', 'NN'), (',', ','), ('And', 'CC'), ('he', 'PRP'), ('laughing', 'VBG'), ('said', 'VBD'), ('to', 'TO'), ('me', 'PRP'), (':', ':'), ('``', '``'), ('Pipe', 'VB'), ('a', 'DT'), ('song', 'NN'), ('about', 'IN'), ('a', 'DT'), ('Lamb', 'NN'), ('!', '.'), (\"''\", \"''\")]\n",
      "[('So', 'RB'), ('I', 'PRP'), ('piped', 'VBD'), ('with', 'IN'), ('merry', 'NNP'), ('cheer', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fields = gutenberg. fileids()\n",
    "sample = gutenberg.raw(\"blake-poems.txt\")\n",
    "tokenized = sent_tokenize(sample)\n",
    "for i in tokenized[:2]:\n",
    "    words = nltk. word_tokenize(i)\n",
    "    tagged = nltk. pos_tag(words)\n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n"
     ]
    }
   ],
   "source": [
    "nltk. help. upenn_tagset( 'NNP' )\n",
    "nltk. help. upenn_tagset( 'RB' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
